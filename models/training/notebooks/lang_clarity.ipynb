{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANG_CLARITY_DATASET1_PATH =  None\n",
    "LANG_CLARITY_DATASET2_PATH = None\n",
    "LANG_CLARITY_MODEL_PATH = None\n",
    "VECTORIZER_PATH = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text : str) -> str:\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    tokens = text.split()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model() -> [RandomForestClassifier, TfidfVectorizer]:\n",
    "    # For pie chart\n",
    "    def autopct_format(values):\n",
    "        def my_format(pct):\n",
    "            total = sum(values)\n",
    "            val = int(round(pct * total / 100.0))\n",
    "\n",
    "            return '{:.1f}%\\n({v:d})'.format(pct, v = val)\n",
    "\n",
    "        return my_format\n",
    "\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "    # Load Ariadne dataset\n",
    "    df1 = pd.read_csv(LANG_CLARITY_DATASET1_PATH)\n",
    "    df1['text'] = df1['text'].astype('str')\n",
    "    df1['rating'] = df1['rating'].astype('str')\n",
    "\n",
    "    # Load Veracookie dataset\n",
    "    df2 = pd.read_csv(LANG_CLARITY_DATASET2_PATH)\n",
    "    df2['text'] = df2['text'].astype('str')\n",
    "    df2['rating'] = df2['rating'].astype('str')\n",
    "\n",
    "    # Combine two datasets\n",
    "    x = pd.concat([df1['text'], df2['text']])\n",
    "    y = pd.concat([df1['rating'], df2['rating']])\n",
    "\n",
    "    # Preprocess texts\n",
    "    x = x.apply(preprocess_text)\n",
    "\n",
    "    # Train-test split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "    # Visualize data in training and test sets\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "\n",
    "    train_dist = y_train.value_counts()\n",
    "    ax1.pie(train_dist, labels = train_dist.index, autopct = autopct_format(train_dist))\n",
    "    ax1.title.set_text(\"Distribution of data \\n in training set\")\n",
    "\n",
    "    test_dist = y_test.value_counts()\n",
    "    ax2.pie(test_dist, labels = test_dist.index, autopct = autopct_format(test_dist))\n",
    "    ax2.title.set_text(\"Distribution of data \\n in testing set\")\n",
    "\n",
    "    # TF-IDF Vectorization\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    x_train_tfidf = vectorizer.fit_transform(x_train)\n",
    "    x_test_tfidf = vectorizer.transform(x_test)\n",
    "\n",
    "    # Random Forest Model\n",
    "    rf_classifier = RandomForestClassifier(n_estimators = 100, random_state = 42)\n",
    "    rf_classifier.fit(x_train_tfidf, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    predictions = rf_classifier.predict(x_test_tfidf)\n",
    "\n",
    "    # Display confusion matrix\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [\"GOOD\", \"BAD\"])\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "\n",
    "    # Display classification report\n",
    "    report = classification_report(y_test, predictions)\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "\n",
    "    return [rf_classifier, vectorizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model : RandomForestClassifier, vectorizer : TfidfVectorizer):\n",
    "    joblib.dump(model, LANG_CLARITY_MODEL_PATH)\n",
    "    joblib.dump(vectorizer, VECTORIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model_path : str, vectorizer_path : str, test_dir : str):\n",
    "    # Load model\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    model = joblib.load(model_path)\n",
    "    vectorizer = joblib.load(vectorizer_path)\n",
    "\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(test_dir)\n",
    "    df['text'] = df['text'].astype('str')\n",
    "    df['rating'] = df['rating'].astype('str')\n",
    "\n",
    "    x = df['text']\n",
    "    y = df['rating']\n",
    "\n",
    "    # Preprocess text\n",
    "    x = x.apply(preprocess_text)\n",
    "\n",
    "    # TF-IDF Vectorization\n",
    "    x_tfidf = vectorizer.transform(x)\n",
    "\n",
    "    # Predictions\n",
    "    predictions = model.predict(x_tfidf)\n",
    "\n",
    "    # Display confusion matrix\n",
    "    cm = confusion_matrix(y, predictions)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [\"GOOD\", \"BAD\"])\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "\n",
    "    # Display classification report\n",
    "    report = classification_report(y, predictions)\n",
    "    print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vectorizer = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = LANG_CLARITY_MODEL_PATH\n",
    "# vectorizer_path = VECTORIZER_PATH\n",
    "# test_dir = LANG_CLARITY_DATASET1_PATH\n",
    "# eval_model(model_path, vectorizer_path, test_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "models-wsg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
